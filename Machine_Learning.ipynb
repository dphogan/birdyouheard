{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Bird Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.preprocessing as preproc\n",
    "import tensorflow.keras.callbacks as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load preprocessed data\n",
    "checklistdir = '/home/dphogan/data_science/species_list'\n",
    "datafile = 'traintestvalid.p'\n",
    "datapath = os.path.join(checklistdir, datafile)\n",
    "(xtrain, ytrain, xvalid, yvalid, xtest, ytest, molabels) = pickle.load(open(datapath, 'rb'))\n",
    "#Save original y-values (turns out these are needed for computing weights below)\n",
    "ztrain = ytrain\n",
    "zvalid = yvalid\n",
    "ztest = ytest\n",
    "#Change output from index lists to one-hot vectors\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "yvalid = keras.utils.to_categorical(yvalid)\n",
    "ytest = keras.utils.to_categorical(ytest)\n",
    "#Add a length-1 dimension to input data  (so it will have same format as a grayscale image)\n",
    "xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1], xtrain.shape[2], 1)\n",
    "xvalid = xvalid.reshape(xvalid.shape[0], xvalid.shape[1], xvalid.shape[2], 1)\n",
    "xtest = xtest.reshape(xtest.shape[0], xtest.shape[1], xtest.shape[2], 1)\n",
    "#Don't accidentally use test data yet\n",
    "xtest = ytest = 0\n",
    "print(np.shape(xtrain))\n",
    "print(np.shape(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute weights (counts, weights by element, weights by class)\n",
    "ctrain = np.bincount(ztrain)\n",
    "cvalid = np.bincount(zvalid)\n",
    "ctest = np.bincount(ztest)\n",
    "\n",
    "wtrain = (float(len(ztrain))/len(ctrain))/ctrain[ztrain]\n",
    "wvalid = (float(len(zvalid))/len(cvalid))/cvalid[zvalid]\n",
    "wtest = (float(len(ztest))/len(ctest))/ctest[ztest]\n",
    "\n",
    "ltrain = (float(len(ztrain))/len(ctrain))/ctrain\n",
    "lvalid = (float(len(zvalid))/len(cvalid))/cvalid\n",
    "ltest = (float(len(ztest))/len(ctest))/ctest\n",
    "\n",
    "dtrain = dict(zip(range(len(ctrain)),ltrain))\n",
    "dvalid = dict(zip(range(len(cvalid)),lvalid))\n",
    "dtest = dict(zip(range(len(ctest)),ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset augmentation\n",
    "datagen = preproc.image.ImageDataGenerator(\n",
    "    data_format = 'channels_last',\n",
    "    width_shift_range = 6,\n",
    "    height_shift_range = 2,\n",
    "    fill_mode = 'constant',\n",
    "    cval = 0\n",
    "    )\n",
    "datagen.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Checkpoint\" callback to save model either after every epoch or after best test accuracy\n",
    "#checkpointpath = 'model_epoch{epoch:02d}.hdf5'\n",
    "checkpointpath = 'model.hdf5'\n",
    "checkpoint = callbacks.ModelCheckpoint(checkpointpath, monitor='val_loss', mode='min',\n",
    "                                       verbose=0, save_best_only=True,\n",
    "                                       save_weights_only=False, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3,5), padding='same', data_format='channels_last', activation='relu', input_shape=np.shape(xtrain)[1:] ))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same', data_format='channels_last'))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3,5), padding='same', data_format='channels_last', activation='relu' ))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2), padding='same', data_format='channels_last'))\n",
    "#model.add(layers.Conv2D(filters=128, kernel_size=(3,5), padding='same', data_format='channels_last', activation='relu' ))\n",
    "#model.add(layers.MaxPooling2D(pool_size=(2,2), padding='same', data_format='channels_last'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=512, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=512, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=np.shape(ytrain)[1], activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "epochs = 25\n",
    "batchsize = 32\n",
    "starttime = time.time()\n",
    "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=batchsize, shuffle=True),\n",
    "                              class_weight=dtrain,\n",
    "                              steps_per_epoch=len(xtrain)/batchsize,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              validation_data=(xvalid, yvalid, wvalid),\n",
    "                              callbacks=[checkpoint]\n",
    "                             )\n",
    "endtime = time.time()\n",
    "print('Time: %.2fs' % (endtime-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model definition (but not weights) to file\n",
    "modeljson = model.to_json()\n",
    "json.dump(modeljson, open('./model.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model to files (json+hdf5)\n",
    "#modeljson = model.to_json()\n",
    "#json.dump(modeljson, open('./model.json', 'w'))\n",
    "#model.save_weights('./model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model from files (json+hdf5)\n",
    "overwritemodelvar = False\n",
    "if overwritemodelvar:\n",
    "    modeljson = json.load(open('model.json', 'r'))\n",
    "    model = models.model_from_json(modeljson)\n",
    "    model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "#Currently used on validation data, but applicable to test data once model is finalized.\n",
    "predictions = model.predict(xvalid, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = np.argmax(predictions,axis=1)==np.argmax(yvalid,axis=1)\n",
    "correctrate = sum(corrects)/len(corrects)\n",
    "print('%.2f' % (correctrate*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
